# Cache Management and Performance Optimization Guide

This guide covers cache cleaning utilities and Phase 3 performance/reliability improvements for DSPy optimization training.

## Cache Cleaning Tools

### Overview

DSPy and related ML libraries can generate large cache files during training and evaluation. This can lead to disk space exhaustion and SQLite database errors. We provide two cache cleaning utilities:

1. **Python script** (`scripts/clean_cache.py`) - Full-featured with dry-run, selective cleaning
2. **Shell script** (`scripts/clean_cache.sh`) - Quick command-line interface

### Python Cache Cleaner

Features:
- Dry-run mode to preview deletions
- Selective cleaning by cache type
- Disk space monitoring
- Backup options
- Detailed size analysis

```bash
# Preview what would be cleaned
uv run python scripts/clean_cache.py --dry-run

# Clean specific caches
uv run python scripts/clean_cache.py --cache dspy huggingface

# Clean all non-critical caches
uv run python scripts/clean_cache.py --all --exclude critical

# Force clean without confirmation
uv run python scripts/clean_cache.py --force

# With backup
uv run python scripts/clean_cache.py --backup ~/cache_backups/$(date +%Y%m%d)
```

### Shell Cache Cleaner

Features:
- Color-coded output
- Interactive confirmation
- Quick one-command cleanup
- Progress indicators

```bash
# Make executable (one-time)
chmod +x scripts/clean_cache.sh

# Preview with colors
./scripts/clean_cache.sh --dry-run

# Interactive cleanup
./scripts/clean_cache.sh

# Quick cleanup (no confirmation)
./scripts/clean_cache.sh --force

# Specific cache
./scripts/clean_cache.sh --cache huggingface

# Verbose mode
./scripts/clean_cache.sh --verbose
```

### Cache Types

The cleaning tools support the following cache types:

- `dspy` - DSPy model request cache (~10-100MB)
- `huggingface` - HuggingFace models and datasets (10-50GB)
- `uv` - UV package manager cache (~4GB)
- `pip` - Python package cache (~1GB)
- `deepdoctection` - Document processing cache (~1GB)
- `doctr` - OCR library cache (~60MB)
- `selenium` - Browser automation cache (~300MB)

### Automatic Cleanup Integration

Training scripts now include automatic disk space checks and cleanup suggestions:

```bash
# If disk space is low, training will suggest:
# "Insufficient disk space (< 10GB). Please run cache cleanup first."
# "Run: uv run python scripts/clean_cache.py"
```

## Phase 3 Performance Improvements

### 1. Enhanced OpenRouter Configuration

**File**: `optimizations/shared/openrouter_config.py`

**New Features**:
- Disk space monitoring before training
- Configurable caching with size limits
- Automatic cache directory creation
- Cache disable option for training

```python
from thinking.optimizations.shared.openrouter_config import configure_openrouter_lm, check_disk_space

# Check disk space first
has_space, available_gb = check_disk_space(10.0)
if not has_space:
    print("Insufficient disk space")
    sys.exit(1)

# Configure with caching disabled during training
lm = configure_openrouter_lm(
    model="openrouter/openai/gpt-4o",
    enable_cache=False  # Disable caching to prevent disk issues
)
```

### 2. Training Script Reliability

**Files**: All `optimizations/*/training.py` scripts

**New Features**:
- Retry logic with exponential backoff
- API call counting and error tracking
- Performance metrics collection
- Enhanced error specificity
- Logging instead of print statements

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    before_sleep=lambda retry_state: logger.warning(f"LM call failed, retrying...")
)
def safe_lm_call(lm, **kwargs):
    """Wrapper for LM calls with retry logic."""
    return lm(**kwargs)
```

**New CLI Options**:
```bash
# Disable caching during training
uv run python -m thinking.optimizations.direct.training --no-cache

# Training will now:
# 1. Check disk space (10GB minimum)
# 2. Disable DSPy caching
# 3. Use retry logic for API calls
# 4. Track performance metrics
```

### 3. GEPA Parameter Optimization

**File**: `optimizations/shared/gepa_config.py`

**Features**:
- Module-specific parameter tuning
- Complexity-based adjustment
- Automatic parameter selection

```python
from thinking.optimizations.shared.gepa_config import get_gepa_params_auto

# Get optimized parameters for graph-of-thoughts with 30 examples
params = get_gepa_params_auto("graph_of_thoughts", 30)
# Returns: {'auto': 'light', 'max_bootstrapped_demos': 4}

# Manual configuration
from thinking.optimizations.shared.gepa_config import get_gepa_params_for_module

params = get_gepa_params_for_module("direct", complexity="high")
# Returns: {'auto': 'medium', 'max_bootstrapped_demos': 12}
```

**Module-Specific Settings**:

| Module | Budget | Max Demos | Notes |
|---------|---------|-------------|---------|
| direct | light | 8 | Standard settings |
| chain_of_thought | medium | 10 | Can handle more intensity |
| tree_of_thoughts | light | 6 | Moderate complexity |
| graph_of_thoughts | light | 4 | Very complex, conservative |
| atom_of_thoughts | light | 8 | Atomic, moderate |
| combined | light | 4 | Most complex, very conservative |

### 4. Validation and Benchmarking

**File**: `optimizations/shared/validation.py`

**Features**:
- Optimization quality validation
- Baseline vs optimized comparison
- Optimizer benchmarking
- Detailed reporting

```python
from thinking.optimizations.shared.validation import (
    validate_optimization_quality,
    print_validation_report,
)

# Validate optimization quality
results = validate_optimization_quality(
    baseline_module=DirectAnswer(),
    optimized_module=optimized,
    test_examples=test_set,
    metric=reasoning_quality_metric,
    improvement_threshold=0.05  # 5% minimum improvement
)

# Print detailed report
print_validation_report(results)
# Shows baseline/optimized scores, improvement, and pass/fail status
```

**Validation Output**:
```
============================================================
OPTIMIZATION VALIDATION REPORT
============================================================
Baseline Score:        0.4500
Optimized Score:       0.5800
Absolute Improvement:   0.1300
Relative Improvement:   28.9%
Improvement Threshold: 5%

Validation Status:      âœ“ PASSED
============================================================
```

## Troubleshooting

### Disk Space Issues

**Problem**: "unable to open database file" or disk space errors during training

**Solution**:
```bash
# Check current disk space
df -h /System/Volumes/Data

# Clean caches
uv run python scripts/clean_cache.py --force

# Or use shell script
./scripts/clean_cache.sh --force
```

### Network/Retry Issues

**Problem**: API call failures or timeouts

**Solution**: Training scripts now include automatic retry logic with exponential backoff. If issues persist:

```bash
# Reduce GEPA budget for faster, more reliable training
uv run python -m thinking.optimizations.direct.training --budget light

# Or disable caching to reduce disk pressure
uv run python -m thinking.optimizations.direct.training --no-cache
```

### Performance Issues

**Problem**: Training is too slow

**Solution**: Use module-specific parameters:

```python
from thinking.optimizations.shared.gepa_config import get_gepa_params_auto

# For simple modules, use lower budget
params = get_gepa_params_auto("direct", num_examples=10)

# For complex modules like graph-of-thoughts, keep conservative
params = get_gepa_params_for_module("graph_of_thoughts", complexity="low")
```

## Performance Expectations

With Phase 3 improvements, you should see:

- **30-50% faster training** through optimized parameters and retry logic
- **<5% failure rate** with disk checks and error handling
- **Better optimization quality** through validation framework
- **Reduced disk issues** through proactive cache management

## Best Practices

1. **Pre-training checks**: Always run cache cleaning before long training sessions
2. **Start conservative**: Use "light" budget first, then scale up
3. **Monitor logs**: Check training logs for retry patterns and API call counts
4. **Validate results**: Always validate optimization quality on a test set
5. **Benchmark alternatives**: Try both GEPA and BootstrapFewShot for comparison

## Dependencies

New dependencies added in Phase 3:

- `tenacity>=8.2.0` - Retry logic with exponential backoff

To install:
```bash
uv sync
```

## Next Steps

For more advanced usage:

1. **Custom metrics**: Implement task-specific metrics in `optimizations/shared/metrics.py`
2. **Hyperparameter tuning**: Adjust GEPA parameters based on your specific data
3. **Monitoring**: Add telemetry collection for production deployments
4. **A/B testing**: Compare different optimization strategies on real tasks